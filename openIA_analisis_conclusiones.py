import pandas as pd
import openai
from typing import List, Union
import os
from datetime import datetime
from dotenv import load_dotenv
import json
import re

# Cargar variables de entorno desde .env
load_dotenv()

# Configurar la clave API de OpenAI desde variable de entorno
openai.api_key = os.getenv("OPENAI_API_KEY")

# Validar que la API key est√© configurada
if not openai.api_key:
    raise ValueError("OPENAI_API_KEY no est√° configurada. Por favor, crea un archivo .env con tu API key.") 

registro_tokens=[]  # Nueva lista para registrar los tokens usados en cada ejecuci√≥n

# Diccionario de precios por modelo (USD por 1K tokens)
PRECIOS_MODELOS = {
    'gpt-4.1': {'input': 2.00, 'output': 8.00},
    'gpt-4.1-mini': {'input': 0.40, 'output': 1.60},
    'gpt-4.1-nano': {'input': 0.10, 'output': 0.40},
    'gpt-4.5-preview': {'input': 75.00, 'output': 150.00},
    'gpt-4o': {'input': 2.50, 'output': 10.00},
    'gpt-4o-mini': {'input': 0.15, 'output': 0.60},
    'gpt-4o-mini-realtime-preview': {'input': 0.60, 'output': 2.40},
    'gpt-4o-realtime-preview': {'input': 5.00, 'output': 20.00},
    'gpt-4o-audio-preview': {'input': 2.50, 'output': 10.00},
    'gpt-4o-mini-audio-preview': {'input': 0.15, 'output': 0.60},
    'gpt-4o-search-preview': {'input': 2.50, 'output': 10.00},
    'gpt-4o-mini-search-preview': {'input': 0.15, 'output': 0.60},
    'o1': {'input': 15.00, 'output': 60.00},
    'o1-pro': {'input': 150.00, 'output': 600.00},
    'o3-pro': {'input': 20.00, 'output': 80.00},
    'o3': {'input': 2.00, 'output': 8.00},
    'o3-deep-research': {'input': 10.00, 'output': 40.00},
    'o4-mini': {'input': 1.10, 'output': 4.40},
    'o4-mini-deep-research': {'input': 2.00, 'output': 8.00},
    'o3-mini': {'input': 1.10, 'output': 4.40},
    'o1-mini': {'input': 1.10, 'output': 4.40},
    'codex-mini-latest': {'input': 1.50, 'output': 6.00},
    'computer-use-preview': {'input': 3.00, 'output': 12.00},
    'gpt-image-1': {'input': 5.00, 'output': 1.25},
}

def call_gpt(prompt: str, modelo: str = "gpt-4.1-nano", max_tokens: int = 1500, temperature: float = 0.7) -> str:
    """
    Llama a la API de OpenAI. Por defecto usa gpt-4o-mini.
    
    Args:
        prompt (str): Texto del prompt a enviar.
        max_tokens (int): M√°ximo de tokens en la respuesta.
        temperature (float): Control de creatividad (0.0-1.0).
    
    Returns:
        str: Respuesta del modelo.
    """
    try:
        response = openai.chat.completions.create(
            model=modelo,
            messages=[
                {"role": "system", "content": """
Eres un analista de datos educativos especializado en la redacci√≥n de informes t√©cnicos profesionales.

CONTEXTO:
Debes redactar secciones espec√≠ficas de un informe sobre resultados de proyectos educativos, bas√°ndote √∫nicamente en los datos proporcionados.

DIRECTRICES ESTRICTAS:

1. OBJETIVIDAD ABSOLUTA:
   - Describe √∫nicamente lo que muestran los datos, sin interpretaciones causales
   - Evita correlaciones no fundamentadas (ej: "X indica √©xito del programa")
   - No atribuyas significado sin evidencia directa
   - Usa lenguaje neutral y descriptivo

2. LENGUAJE PROFESIONAL:
   - Emplea terminolog√≠a t√©cnica apropiada
   - Redacta en tercera persona
   - Utiliza voz pasiva cuando sea pertinente
   - Mant√©n un tono formal y acad√©mico

3. PROHIBICIONES EXPL√çCITAS:
   - NO inferir causalidad sin evidencia
   - NO hacer juicios de valor sobre los datos
   - NO relacionar variables demogr√°ficas con √©xito/fracaso
   - NO incluir recomendaciones no solicitadas
   - NO usar adjetivos valorativos (exitoso, deficiente, prometedor)

4. FORMATO DE RESPUESTA:
   - P√°rrafos concisos de 3-5 oraciones
   - Incluye datos espec√≠ficos cuando sea relevante (porcentajes, cifras)

EJEMPLO DE REDACCI√ìN APROPIADA:
Incorrecto: "La alta participaci√≥n femenina (70%) demuestra el √©xito del programa"
Correcto: "La distribuci√≥n por g√©nero muestra una participaci√≥n del 70% de mujeres y 30% de hombres"

                 """},
                {"role": "user", "content": prompt}
            ],
            max_tokens=max_tokens,
            temperature=temperature
        )
        result = response.choices[0].message.content.strip()

        usage = response.usage
        input_tokens = usage.prompt_tokens
        output_tokens = usage.completion_tokens

        # Determinar el modelo base para buscar en el diccionario (por si el nombre tiene sufijos de fecha)
        modelo_base = modelo.split("-")[0] if modelo not in PRECIOS_MODELOS else modelo
        if modelo not in PRECIOS_MODELOS:
            # Buscar coincidencia parcial si el modelo tiene sufijo de fecha
            for key in PRECIOS_MODELOS:
                if modelo.startswith(key):
                    modelo_base = key
                    break
        else:
            modelo_base = modelo
        precios = PRECIOS_MODELOS.get(modelo_base, {'input': 0, 'output': 0})
        
        cost_usd = (input_tokens * precios['input'] + output_tokens * precios['output']) / 1000000

        # Registrar informaci√≥n de tokens en la lista registro_tokens
        registro_tokens.append({
            'fecha_hora': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'modelo': modelo,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'costo_usd': cost_usd,
        })

        return result
    



    except openai.OpenAIError as e:
        return f"Error en la API de OpenAI: {str(e)}"
    except Exception as e:
        return f"Error inesperado: {str(e)}"

def analyze_dataframe(df: pd.DataFrame, pregunta: str = "", matriz: bool = False, tokens: int = 1000) -> str:
    """
    Analiza un DataFrame y obtiene conclusiones.
    
    Args:
        df (pd.DataFrame): DataFrame a analizar.
        pregunta (str): Pregunta asociada a los datos del DataFrame.
    
    Returns:
        str: Conclusi√≥n generada por el modelo.
    """
    # Convertir DataFrame a string (primeras filas y descripci√≥n)

    json_str = df.to_json(orient="records", lines=False, force_ascii=False)

    if matriz is True:

        base_prompt = f"""
        Contexto: '{pregunta}'.

        Realiza un an√°lisis conciso de m√°ximo 3 l√≠neas. 
        Da una conclusion a partir de tendencias, valores at√≠picos o patrones relevantes para el lector.
        S√© claro, preciso y enf√≥cate en los aspectos m√°s significativos.

        No dees valores que no puedan ser observados en los datos y tampoco dees resultados de calculos
        .

        json a analizar:
        {json_str}

        Formato de salida: No uses markdown, solo texto plano. No uses titulos, solo p√°rrafos. No uses emojis. No uses saltos de linea. Porcentajes con 1 decimal.
        """
    else:

        base_prompt = f"""
        Tema: '{pregunta}'.

        Realiza un an√°lisis conciso de m√°ximo 3 l√≠neas. 
        Identifica y explica las caracter√≠sticas m√°s importantes o destacadas de los datos, como tendencias, valores at√≠picos, distribuciones o patrones relevantes.
        S√© claro, preciso y enf√≥cate en los aspectos m√°s significativos.
        
        
        Json a analizar:
        {json_str}

        Formato de salida: No uses markdown, solo texto plano. No uses titulos, solo p√°rrafos. No uses emojis. No uses saltos de linea. Porcentajes con 1 decimal.
        """
    
    return call_gpt(base_prompt, max_tokens=tokens)

def analyze_list(data_list: List[Union[int, float, str]], proyectos: pd.DataFrame = None, introduccion: str = "", tokens: int = 2000) -> str:
    """
    Analiza una lista y obtiene conclusiones.
    
    Args:
        data_list (List): Lista de datos a analizar.
        introduccion (str): Introducci√≥n o contexto del an√°lisis.
    
    Returns:
        str: Conclusi√≥n generada por el modelo.
    """
    # Convertir lista a string
    list_str = ", ".join(str(item) for item in data_list)

    if proyectos is not None:
        json_str = proyectos.to_json(orient="records", lines=False, force_ascii=False)
    else:
        json_str = ""
    
    # Construir prompt base
    base_prompt = f"""
    Bas√°ndote en la siguiente introducci√≥n y las conclusiones parciales proporcionadas, 
    genera un resumen ejecutivo de todo el documento en un m√°ximo de 3-4 p√°rrafos. 
    Nombra a todos los proyectos y que actividades se realizaron en cada uno de ellos.
    Sintetiza la informaci√≥n, destacando los hallazgos clave, el impacto general del proyecto y cualquier
    recomendaci√≥n o insight relevante para futuros proyectos. Mant√©n el tono profesional y orientado a resultados educativos

    Proyectos:
    {json_str}

    Introduccion:
    {introduccion}

    Las conclusiones parciales son: {list_str}.

    Formato de salida: No uses markdown, solo texto plano. No uses titulos, solo p√°rrafos. No uses emojis. No uses saltos de linea.
    """
    
    return call_gpt(base_prompt, max_tokens=tokens)

def insight_parcial(data_list: List[Union[int, float, str]], pregunta: str = "", tokens: int = 1000) -> str:

    """
    Analiza una lista y obtiene insights claves.
    
    Args:
        data_list (List): Lista de datos a analizar.
    
    Returns:
        str: Conclusi√≥n generada por el modelo.
    """
    # Convertir lista a string
    list_str = ", ".join(str(item) for item in data_list)
    
    # Construir prompt base
    base_prompt = f"""
    Basandote en las conclusiones parciales a la {pregunta}. 
    Genera maximo 3 insight a modo de resumen para que vuelvan a ser interpretados por el modelo.

    Conclusiones parciales:
    {list_str} """
    
    return call_gpt(base_prompt, max_tokens=tokens)


def insight_list(data_list: List[Union[int, float, str]], proyectos: pd.DataFrame = None, introduccion: str = "", tokens: int = 2000) -> str:
    """
    Analiza una lista y obtiene insights claves pero extensos, devolviendo un JSON v√°lido.
    
    Args:
        data_list (List): Lista de datos a analizar.
        proyectos (pd.DataFrame): DataFrame con informaci√≥n de proyectos.
        introduccion (str): Introducci√≥n o contexto del an√°lisis.
        tokens (int): M√°ximo de tokens para la respuesta.
    
    Returns:
        str: JSON v√°lido con insights generados por el modelo.
    """
    # Convertir lista a string
    list_str = ", ".join(str(item) for item in data_list)

    if proyectos is not None:
        json_str = proyectos.to_json(orient="records", lines=False, force_ascii=False)
    else:
        json_str = ""
    
    # Construir prompt mejorado con instrucciones m√°s estrictas
    base_prompt = f"""
Bas√°ndote en la siguiente introducci√≥n, informaci√≥n de proyectos y conclusiones parciales, genera un resumen estructurado en formato JSON que destaque los principales hallazgos e insights por dimensi√≥n o categor√≠a.

üëâ Introducci√≥n:
{introduccion}

üëâ Proyectos:
{json_str}

üëâ Conclusiones parciales:
{list_str}

üßæ Formato de salida EXACTO (devuelve √öNICAMENTE este JSON, sin texto adicional):
{{    
  "Contexto General del Diagn√≥stico": [
    "Insight 1",
    "Insight 2",
    "Insight 3"
  ],
  "Hallazgos Clave y Correlaciones Relevantes": {{
    "Nombre de la categor√≠a": [
      "Insight 1",
      "Insight 2",
      "Insight 3",
      "Implicaci√≥n: texto aqu√≠"
    ]
  }},
  "Retos Priorizados Identificados": [
    {{
      "Eje": "Nombre del eje",
      "Reto": "Descripci√≥n del reto",
      "Relevancia": "Raz√≥n por la cual es importante"
    }}
  ],
  "Otras Secciones Relevantes": {{
    "T√≠tulo de la secci√≥n": [
      "Insight 1",
      "Insight 2",
      "Insight 3"
    ]
  }},
  "Relevancia del Programa": [
    "Punto 1 sobre impacto del programa",
    "Punto 2",
    "Punto 3"
  ]
}}

‚úÖ INSTRUCCIONES CR√çTICAS PARA JSON V√ÅLIDO:
- Tu respuesta DEBE ser √öNICAMENTE el objeto JSON, sin markdown, sin explicaciones, sin comentarios
- NO uses comillas simples, SOLO comillas dobles
- NO pongas comas despu√©s del √∫ltimo elemento de arrays o objetos
- NO uses saltos de l√≠nea dentro de los strings (usa espacios en su lugar)
- Si necesitas usar comillas dentro de un string, esc√°palas con backslash
- NO incluyas los caracteres ```json o ``` al inicio o final
- Si alguna secci√≥n no aplica, om√≠tela completamente
- Aseg√∫rate de cerrar todos los corchetes y llaves correctamente
- Los valores num√©ricos NO deben estar entre comillas

RECUERDA: Solo devuelve el JSON, absolutamente NADA m√°s.
"""
    
    # Llamar al modelo GPT
    respuesta_gpt = call_gpt(base_prompt, max_tokens=tokens)
    
    # Limpiar y validar el JSON
    json_limpio = limpiar_json_respuesta(respuesta_gpt)
    
    return json_limpio


def limpiar_json_respuesta(texto_respuesta: str) -> str:
    """
    Limpia y valida el JSON devuelto por GPT para asegurar que sea v√°lido.
    
    Args:
        texto_respuesta (str): Respuesta cruda del modelo GPT.
    
    Returns:
        str: JSON v√°lido como string.
    """
    try:
        # Paso 1: Extraer solo el JSON del texto
        # Buscar el patr√≥n del JSON
        match = re.search(r'\{.*\}', texto_respuesta, re.DOTALL)
        
        if not match:
            print("‚ö†Ô∏è No se encontr√≥ JSON en la respuesta")
            return json.dumps({
                "error": "No se pudo generar el JSON",
                "respuesta_original": texto_respuesta[:500]
            })
        
        json_str = match.group(0)
        
        # Paso 2: Limpiezas b√°sicas
        # Eliminar markdown si existe
        json_str = re.sub(r'```json\s*', '', json_str)
        json_str = re.sub(r'```\s*', '', json_str)
        
        # Paso 3: Intentar parsear directamente
        try:
            parsed = json.loads(json_str)
            # Si funciona, devolverlo como string JSON v√°lido
            return json.dumps(parsed, ensure_ascii=False, indent=2)
        except json.JSONDecodeError:
            pass
        
        # Paso 4: Aplicar correcciones si falla el parseo inicial
        # Eliminar saltos de l√≠nea problem√°ticos dentro de strings
        json_str = re.sub(r'("(?:[^"\\]|\\.)*")', 
                         lambda m: m.group(0).replace('\n', ' ').replace('\r', ''), 
                         json_str)
        
        # Eliminar comas antes de } o ]
        json_str = re.sub(r',\s*([}\]])', r'\1', json_str)
        
        # Corregir comillas mal escapadas
        json_str = re.sub(r'\\([^"\\/bfnrtu])', r'\1', json_str)
        
        # Eliminar caracteres invisibles problem√°ticos
        json_str = ''.join(char for char in json_str if ord(char) >= 32 or char in '\n\r\t')
        
        # Paso 5: Segundo intento de parseo
        try:
            parsed = json.loads(json_str)
            return json.dumps(parsed, ensure_ascii=False, indent=2)
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è Error al parsear JSON despu√©s de limpieza: {e}")
            
            # Paso 6: Intento m√°s agresivo de reparaci√≥n
            try:
                # Reemplazar valores problem√°ticos
                json_str = re.sub(r':\s*undefined', ': null', json_str)
                json_str = re.sub(r':\s*NaN', ': null', json_str)
                
                # Asegurar que los strings est√©n entre comillas
                json_str = re.sub(r'([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', json_str)
                
                parsed = json.loads(json_str)
                return json.dumps(parsed, ensure_ascii=False, indent=2)
                
            except:
                # Paso 7: Si todo falla, devolver una estructura por defecto
                print("‚ùå No se pudo reparar el JSON. Devolviendo estructura por defecto.")
                return json.dumps({
                    "Contexto General del Diagn√≥stico": [
                        "Error al procesar la respuesta del modelo"
                    ],
                    "Hallazgos Clave y Correlaciones Relevantes": {
                        "Estado": ["Revisar manualmente la salida del modelo"]
                    },
                    "Retos Priorizados Identificados": [
                        {
                            "Eje": "Procesamiento",
                            "Reto": "Error en formato JSON",
                            "Relevancia": "Requiere revisi√≥n manual"
                        }
                    ],
                    "respuesta_original_truncada": texto_respuesta[:500]
                }, ensure_ascii=False, indent=2)
                
    except Exception as e:
        print(f"‚ùå Error inesperado: {e}")
        return json.dumps({
            "error": str(e),
            "mensaje": "Error procesando la respuesta"
        })